{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucfji8U8Urjz"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Before we start implementing the pipeline, let's import all the libraries we need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myoQZ72zUrj0",
        "outputId": "3b28688e-3b88-4d59-bb3d-e32ee7261945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m876.5/876.5 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-nlp 0.18.1 requires keras-hub==0.18.1, but you have keras-hub 0.21.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-nlp 0.18.1 requires keras-hub==0.18.1, but you have keras-hub 0.21.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade rouge-score\n",
        "!pip install -q --upgrade keras-hub\n",
        "!pip install -q --upgrade keras  # Upgrade to Keras 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3BugDyrUrj1"
      },
      "outputs": [],
      "source": [
        "import keras_hub\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "import keras\n",
        "from keras import ops\n",
        "\n",
        "import tensorflow.data as tf_data\n",
        "from tensorflow_text.tools.wordpiece_vocab import (\n",
        "    bert_vocab_from_dataset as bert_vocab,\n",
        ")\n",
        "import pathlib\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"keras_hub version:\", keras_hub.__version__)\n",
        "print(\"keras version:\", keras.__version__)\n",
        "print(\"tensorflow version:\", tf.__version__)\n",
        "print(\"tensorflow_text version:\", tf.__version__) # Accessing version through tensorflow\n",
        "print(\"requests version:\", requests.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6YONbYSqiKe",
        "outputId": "e63402e6-b03e-43e7-f7f8-7cb4913e8df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keras_hub version: 0.21.1\n",
            "keras version: 3.10.0\n",
            "tensorflow version: 2.18.0\n",
            "tensorflow_text version: 2.18.0\n",
            "requests version: 2.32.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KayIKSRVUrj2"
      },
      "source": [
        "Let's also define our parameters/hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwrG8D7KUrj2"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 40\n",
        "MAX_SEQUENCE_LENGTH = 40\n",
        "ENG_VOCAB_SIZE = 15000\n",
        "IND_VOCAB_SIZE = 15000\n",
        "\n",
        "EMBED_DIM = 256\n",
        "INTERMEDIATE_DIM = 2048\n",
        "NUM_HEADS = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs5wN8pOUrj3"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "Please download the dataset via this link: https://www.manythings.org/anki/\n",
        "\n",
        "Select the Indonesian-English dataset and save it to your personal drive."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jzU97SdZqoY",
        "outputId": "27822603-2e67-42ba-ec8b-fde887c5c650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skJ41dCGi639",
        "outputId": "dca21195-ea66-41ec-f742-5b12e693248e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path final: /content/ind-eng_extracted/ind.txt\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Download manual ke /content/\n",
        "local_zip_path = \"/content/drive/MyDrive/Colab-Notebooks/ind-eng.zip\"\n",
        "\n",
        "# Step 2: Extract to its own folder\n",
        "extract_dir = \"/content/ind-eng_extracted\"\n",
        "if not os.path.exists(extract_dir):\n",
        "    print(\"Extracting...\")\n",
        "    with zipfile.ZipFile(local_zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "\n",
        "# Step 3: Akses file ind.txt\n",
        "text_file = pathlib.Path(extract_dir) / \"ind.txt\"\n",
        "print(f\"Path final: {text_file}\")\n",
        "assert text_file.exists(), \"File tidak ditemukan!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGbCHnf_Urj3"
      },
      "source": [
        "## Parsing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw2rn0qLUrj4"
      },
      "outputs": [],
      "source": [
        "text_pairs = []\n",
        "\n",
        "with open(text_file, encoding=\"utf-8\") as f:\n",
        "    lines = f.read().strip().split(\"\\n\")\n",
        "\n",
        "for line in lines:\n",
        "    parts = line.split(\"\\t\")\n",
        "    if len(parts) >= 2:\n",
        "        eng = parts[0].strip().lower()\n",
        "        ind = parts[1].strip().lower()\n",
        "        text_pairs.append((eng, ind))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsvivtmxUrj4"
      },
      "source": [
        "Here's what our sentence pairs look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-aQhE9PUrj4",
        "outputId": "3459918f-8552-4269-99d9-da0879ace9aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('did anything interesting happen while i was gone?', 'apakah sesuatu yang menarik terjadi ketika aku pergi?')\n",
            "(\"you aren't my mother.\", 'kamu bukan ibuku.')\n",
            "('i am not a student.', 'aku bukan siswa.')\n",
            "('she is not young.', 'dia tidak muda.')\n",
            "('are we done yet?', 'apa kita masih belum selesai?')\n"
          ]
        }
      ],
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P97x2NwIUrj4"
      },
      "source": [
        "Now, let's split the sentence pairs into a training set, a validation set,\n",
        "and a test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNE9QP9IUrj4",
        "outputId": "dd10b511-8617-41b2-b893-3d225646c3d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14881 total pairs\n",
            "10417 training pairs\n",
            "2232 validation pairs\n",
            "2232 test pairs\n"
          ]
        }
      ],
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhTgrqw5Urj5"
      },
      "source": [
        "## Tokenizing the Data\n",
        "\n",
        "We'll create two tokenizers — one for the source language (Indonesian), and another for the target language (English). To tokenize the text, we’ll use `keras_hub.tokenizers.WordPieceTokenizer`.\n",
        "\n",
        "`WordPieceTokenizer` uses a WordPiece vocabulary and provides functionality for both breaking text into tokens and reconstructing text from token sequences.\n",
        "\n",
        "Before setting up the tokenizers, we first need to train them using our dataset. WordPiece is a subword-based tokenization algorithm. Training it on a corpus results in a vocabulary composed of subwords.\n",
        "\n",
        "Subword tokenization offers a balance between:\n",
        "\n",
        "* **Word-level tokenization**, which usually requires a very large vocabulary to cover all possible words, and\n",
        "* **Character-level tokenization**, which often loses semantic information since individual characters don't convey meaning on their own.\n",
        "\n",
        "Fortunately, KerasHub simplifies the process of training a WordPiece tokenizer with the `keras_hub.tokenizers.compute_word_piece_vocabulary` utility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji8zElZeUrj5"
      },
      "outputs": [],
      "source": [
        "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
        "    word_piece_ds = tf_data.Dataset.from_tensor_slices(text_samples)\n",
        "    vocab = keras_hub.tokenizers.compute_word_piece_vocabulary(\n",
        "        word_piece_ds.batch(1000).prefetch(2),\n",
        "        vocabulary_size=vocab_size,\n",
        "        reserved_tokens=reserved_tokens,\n",
        "    )\n",
        "    return vocab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYEN4ucZUrj5"
      },
      "source": [
        "Every vocabulary has a few special, reserved tokens. We have four such tokens:\n",
        "\n",
        "- `\"[PAD]\"` - Padding token. Padding tokens are appended to the input sequence\n",
        "length when the input sequence length is shorter than the maximum sequence length.\n",
        "- `\"[UNK]\"` - Unknown token.\n",
        "- `\"[START]\"` - Token that marks the start of the input sequence.\n",
        "- `\"[END]\"` - Token that marks the end of the input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UONMf_WSUrj5"
      },
      "outputs": [],
      "source": [
        "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "eng_samples = [text_pair[0] for text_pair in train_pairs]\n",
        "eng_vocab = train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)\n",
        "\n",
        "ind_samples = [text_pair[1] for text_pair in train_pairs]\n",
        "ind_vocab = train_word_piece(ind_samples, IND_VOCAB_SIZE, reserved_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxavnis0Urj5"
      },
      "source": [
        "Let's see some tokens!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co49zbfTUrj6",
        "outputId": "bb5734b8-46f5-4e8c-b14d-e8ca650bd720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Tokens:  ['ll', '##d', 'did', 've', '##y', 'where', 'about', 'they', 'one', 'time']\n",
            "Indo Tokens:  ['punya', 'sedang', 'sangat', 'makan', 'mereka', 'sini', 'mana', '##a', '##lah', 'banyak']\n"
          ]
        }
      ],
      "source": [
        "print(\"English Tokens: \", eng_vocab[100:110])\n",
        "print(\"Indo Tokens: \", ind_vocab[100:110])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzeKQmiIUrj6"
      },
      "source": [
        "Now, let's define the tokenizers. We will configure the tokenizers with the\n",
        "the vocabularies trained above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pon35ogwUrj6"
      },
      "outputs": [],
      "source": [
        "eng_tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=eng_vocab, lowercase=False\n",
        ")\n",
        "ind_tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=ind_vocab, lowercase=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzBRpZjPUrj6"
      },
      "source": [
        "Let's try and tokenize a sample from our dataset! To verify whether the text has\n",
        "been tokenized correctly, we can also detokenize the list of tokens back to the\n",
        "original text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBCqc4P9Urj6",
        "outputId": "e03c0052-836e-4554-97bd-1afd53c1d441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English sentence:  do you happen to have matches?\n",
            "Tokens:  tf.Tensor([ 67  57 466  59  70  38 313 771  25], shape=(9,), dtype=int32)\n",
            "Recovered text after detokenizing:  do you happen to have matches ?\n",
            "\n",
            "Indo sentence:  apa kamu mempunyai korek?\n",
            "Tokens:  tf.Tensor([ 71  66  42 209 146 349 819  40 378 288  29], shape=(11,), dtype=int32)\n",
            "Recovered text after detokenizing:  apa kamu mempunyai korek ?\n"
          ]
        }
      ],
      "source": [
        "eng_input_ex = text_pairs[0][0]\n",
        "eng_tokens_ex = eng_tokenizer.tokenize(eng_input_ex)\n",
        "print(\"English sentence: \", eng_input_ex)\n",
        "print(\"Tokens: \", eng_tokens_ex)\n",
        "print(\n",
        "    \"Recovered text after detokenizing: \",\n",
        "    eng_tokenizer.detokenize(eng_tokens_ex),\n",
        ")\n",
        "\n",
        "print()\n",
        "\n",
        "ind_input_ex = text_pairs[0][1]\n",
        "ind_tokens_ex = ind_tokenizer.tokenize(ind_input_ex)\n",
        "print(\"Indo sentence: \", ind_input_ex)\n",
        "print(\"Tokens: \", ind_tokens_ex)\n",
        "print(\n",
        "    \"Recovered text after detokenizing: \",\n",
        "    ind_tokenizer.detokenize(ind_tokens_ex),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TvkQ_8MUrj6"
      },
      "source": [
        "## Preparing the Datasets\n",
        "\n",
        "Next, we'll prepare our datasets for training. At each training step, the model aims to predict the next word (N+1 and beyond) using the input sentence and the current portion of the target sentence, from word 0 to N.\n",
        "\n",
        "Therefore, each training sample will consist of a tuple `(inputs, targets)`, where:\n",
        "\n",
        "* `inputs` is a dictionary containing two keys: `encoder_inputs` and `decoder_inputs`. `encoder_inputs` refers to the tokenized input sentence in Indonesian, while `decoder_inputs` contains the target English sentence up to the current word — i.e., words 0 to N — which the model will use to predict the following word(s).\n",
        "* `targets` is the English sentence shifted by one position, providing the expected next word that the model is supposed to learn to predict.\n",
        "\n",
        "We'll include special tokens `\"[START]\"` and `\"[END]\"` around the tokenized Indonesian input sentence. The input will also be padded to a fixed length, which can be conveniently handled using `keras_nlp.layers.StartEndPacker`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW-1I_3gUrj6"
      },
      "outputs": [],
      "source": [
        "def preprocess_batch(eng, ind):\n",
        "    batch_size = ops.shape(ind)[0]\n",
        "\n",
        "    eng = eng_tokenizer(eng)\n",
        "    ind = ind_tokenizer(ind)\n",
        "\n",
        "    # Pad `eng` to `MAX_SEQUENCE_LENGTH`.\n",
        "    eng_start_end_packer = keras_hub.layers.StartEndPacker(\n",
        "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "        pad_value=eng_tokenizer.token_to_id(\"[PAD]\"),\n",
        "    )\n",
        "    eng = eng_start_end_packer(eng)\n",
        "\n",
        "    # Add special tokens (`\"[START]\"` and `\"[END]\"`) to `ind` and pad it as well.\n",
        "    ind_start_end_packer = keras_hub.layers.StartEndPacker(\n",
        "        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
        "        start_value=ind_tokenizer.token_to_id(\"[START]\"),\n",
        "        end_value=ind_tokenizer.token_to_id(\"[END]\"),\n",
        "        pad_value=ind_tokenizer.token_to_id(\"[PAD]\"),\n",
        "    )\n",
        "    ind = ind_start_end_packer(ind)\n",
        "\n",
        "    return (\n",
        "        {\n",
        "            \"encoder_inputs\": eng,\n",
        "            \"decoder_inputs\": ind[:, :-1],\n",
        "        },\n",
        "        ind[:, 1:],\n",
        "    )\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, ind_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    ind_texts = list(ind_texts)\n",
        "    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, ind_texts))\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf_data.AUTOTUNE)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjlh8TIiUrj6"
      },
      "source": [
        "Let's take a quick look at the sequence shapes\n",
        "(we have batches of 64 pairs, and all sequences are 40 steps long):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUwYU5stUrj7",
        "outputId": "bd9fa4f2-361a-4da5-edad-a535d42b5d95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 40)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 40)\n",
            "targets.shape: (64, 40)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DZXnu57Urj7"
      },
      "source": [
        "\n",
        "## Building the Model\n",
        "\n",
        "Now we’re moving on to the exciting part — building our model!\n",
        "\n",
        "First, we need an embedding layer, which assigns a vector representation to each token in the input sequence. This layer can start with random initialization. We also require a positional embedding layer that encodes the position of each word within the sequence. Typically, the token embeddings and positional embeddings are added together. KerasHub provides a convenient layer for this: `keras_hub.layers.TokenAndPositionEmbedding`, which handles both token and position embedding for us.\n",
        "\n",
        "Our sequence-to-sequence Transformer model is composed of two main components:\n",
        "\n",
        "* `keras_hub.layers.TransformerEncoder`\n",
        "* `keras_hub.layers.TransformerDecoder`\n",
        "\n",
        "The process works as follows:\n",
        "\n",
        "1. The input sequence in Indonesian is passed to the `TransformerEncoder`, which generates a contextualized representation of the sentence.\n",
        "2. This encoded output, along with the English target sequence so far (from token 0 to N), is then passed into the `TransformerDecoder`.\n",
        "3. The decoder attempts to predict the next token(s) in the English sentence (token N+1 and beyond).\n",
        "\n",
        "A crucial part of making this architecture work is **causal masking**. Since the `TransformerDecoder` processes the entire sequence at once, we need to ensure it doesn’t access information from future tokens (i.e., tokens beyond N when predicting token N+1). This is where causal masking comes in — it ensures the decoder only attends to previous or current tokens. Thankfully, causal masking is enabled by default in `keras_hub.layers.TransformerDecoder`.\n",
        "\n",
        "Another important aspect is **masking the padding tokens** (such as `\"[PAD]\"`). We can achieve this by setting `mask_zero=True` in the `TokenAndPositionEmbedding` layer. This masking will then automatically be respected throughout the rest of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No1JR87NUrj7"
      },
      "outputs": [],
      "source": [
        "# Encoder\n",
        "encoder_inputs = keras.Input(shape=(None,), name=\"encoder_inputs\")\n",
        "\n",
        "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=ENG_VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        ")(encoder_inputs)\n",
        "\n",
        "encoder_outputs = keras_hub.layers.TransformerEncoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(inputs=x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = keras.Input(shape=(None,), name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
        "\n",
        "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=IND_VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        ")(decoder_inputs)\n",
        "\n",
        "x = keras_hub.layers.TransformerDecoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "decoder_outputs = keras.layers.Dense(IND_VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "decoder = keras.Model(\n",
        "    [\n",
        "        decoder_inputs,\n",
        "        encoded_seq_inputs,\n",
        "    ],\n",
        "    decoder_outputs,\n",
        ")\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs],\n",
        "    decoder_outputs,\n",
        "    name=\"transformer\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhIWe-fVUrj8"
      },
      "source": [
        "\n",
        "## Training the Model\n",
        "\n",
        "To monitor the training process, we'll use accuracy as a simple metric to track performance on the validation set. While accuracy offers a quick overview, it's worth noting that machine translation tasks are more commonly evaluated using metrics like **BLEU** or **ROUGE**.\n",
        "\n",
        "However, these advanced metrics require converting the model’s output probabilities back into actual text — a process known as decoding. Since text generation is computationally intensive, it's generally not advisable to perform this step during training.\n",
        "\n",
        "In this example, we only train the model for a single epoch. But in practice, achieving meaningful translation quality will require training for **at least 10 epochs** to allow the model to properly converge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TCBldZh7Urj8",
        "outputId": "dd10d960-dceb-450c-e223-b125597d5ad4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"transformer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ token_and_position… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m3,850,240\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mTokenAndPositionE…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m1,315,072\u001b[0m │ token_and_positi… │\n",
              "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_5        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m9,283,992\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m15000\u001b[0m)            │            │ transformer_enco… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ token_and_position… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,850,240</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionE…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,315,072</span> │ token_and_positi… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_5        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">9,283,992</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)            │            │ transformer_enco… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,449,304\u001b[0m (55.12 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,449,304</span> (55.12 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,449,304\u001b[0m (55.12 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,449,304</span> (55.12 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 151ms/step - accuracy: 0.7674 - loss: 2.7666 - val_accuracy: 0.8200 - val_loss: 1.0703\n",
            "Epoch 2/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - accuracy: 0.8223 - loss: 1.0394 - val_accuracy: 0.8243 - val_loss: 0.9653\n",
            "Epoch 3/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.8263 - loss: 0.9503 - val_accuracy: 0.8278 - val_loss: 0.9120\n",
            "Epoch 4/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.8297 - loss: 0.9009 - val_accuracy: 0.8402 - val_loss: 0.8437\n",
            "Epoch 5/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 64ms/step - accuracy: 0.8421 - loss: 0.8268 - val_accuracy: 0.8487 - val_loss: 0.7917\n",
            "Epoch 6/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 65ms/step - accuracy: 0.8506 - loss: 0.7672 - val_accuracy: 0.8570 - val_loss: 0.7382\n",
            "Epoch 7/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.8586 - loss: 0.7112 - val_accuracy: 0.8607 - val_loss: 0.7058\n",
            "Epoch 8/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 64ms/step - accuracy: 0.8669 - loss: 0.6559 - val_accuracy: 0.8678 - val_loss: 0.6687\n",
            "Epoch 9/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.8750 - loss: 0.6034 - val_accuracy: 0.8732 - val_loss: 0.6318\n",
            "Epoch 10/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 63ms/step - accuracy: 0.8814 - loss: 0.5597 - val_accuracy: 0.8785 - val_loss: 0.6085\n",
            "Epoch 11/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 64ms/step - accuracy: 0.8886 - loss: 0.5177 - val_accuracy: 0.8815 - val_loss: 0.5953\n",
            "Epoch 12/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 64ms/step - accuracy: 0.8941 - loss: 0.4824 - val_accuracy: 0.8834 - val_loss: 0.5895\n",
            "Epoch 13/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9001 - loss: 0.4477 - val_accuracy: 0.8860 - val_loss: 0.5776\n",
            "Epoch 14/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.9054 - loss: 0.4182 - val_accuracy: 0.8856 - val_loss: 0.5829\n",
            "Epoch 15/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - accuracy: 0.9105 - loss: 0.3898 - val_accuracy: 0.8873 - val_loss: 0.5914\n",
            "Epoch 16/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 62ms/step - accuracy: 0.9155 - loss: 0.3603 - val_accuracy: 0.8890 - val_loss: 0.5838\n",
            "Epoch 17/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.9207 - loss: 0.3341 - val_accuracy: 0.8904 - val_loss: 0.5899\n",
            "Epoch 18/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - accuracy: 0.9248 - loss: 0.3111 - val_accuracy: 0.8914 - val_loss: 0.5985\n",
            "Epoch 19/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.9296 - loss: 0.2857 - val_accuracy: 0.8917 - val_loss: 0.6145\n",
            "Epoch 20/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.9350 - loss: 0.2611 - val_accuracy: 0.8932 - val_loss: 0.6133\n",
            "Epoch 21/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - accuracy: 0.9395 - loss: 0.2372 - val_accuracy: 0.8939 - val_loss: 0.6274\n",
            "Epoch 22/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - accuracy: 0.9437 - loss: 0.2179 - val_accuracy: 0.8927 - val_loss: 0.6463\n",
            "Epoch 23/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - accuracy: 0.9481 - loss: 0.1986 - val_accuracy: 0.8932 - val_loss: 0.6554\n",
            "Epoch 24/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - accuracy: 0.9515 - loss: 0.1817 - val_accuracy: 0.8938 - val_loss: 0.6685\n",
            "Epoch 25/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.9555 - loss: 0.1642 - val_accuracy: 0.8932 - val_loss: 0.6841\n",
            "Epoch 26/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - accuracy: 0.9592 - loss: 0.1485 - val_accuracy: 0.8944 - val_loss: 0.6951\n",
            "Epoch 27/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - accuracy: 0.9622 - loss: 0.1358 - val_accuracy: 0.8944 - val_loss: 0.7165\n",
            "Epoch 28/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - accuracy: 0.9657 - loss: 0.1223 - val_accuracy: 0.8939 - val_loss: 0.7245\n",
            "Epoch 29/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 64ms/step - accuracy: 0.9675 - loss: 0.1150 - val_accuracy: 0.8935 - val_loss: 0.7370\n",
            "Epoch 30/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - accuracy: 0.9700 - loss: 0.1058 - val_accuracy: 0.8947 - val_loss: 0.7433\n",
            "Epoch 31/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.9720 - loss: 0.0980 - val_accuracy: 0.8942 - val_loss: 0.7465\n",
            "Epoch 32/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.9746 - loss: 0.0890 - val_accuracy: 0.8952 - val_loss: 0.7649\n",
            "Epoch 33/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.9758 - loss: 0.0835 - val_accuracy: 0.8957 - val_loss: 0.7756\n",
            "Epoch 34/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.9773 - loss: 0.0793 - val_accuracy: 0.8952 - val_loss: 0.7903\n",
            "Epoch 35/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - accuracy: 0.9785 - loss: 0.0744 - val_accuracy: 0.8958 - val_loss: 0.7825\n",
            "Epoch 36/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - accuracy: 0.9800 - loss: 0.0703 - val_accuracy: 0.8941 - val_loss: 0.8016\n",
            "Epoch 37/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.9798 - loss: 0.0705 - val_accuracy: 0.8958 - val_loss: 0.8040\n",
            "Epoch 38/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - accuracy: 0.9820 - loss: 0.0632 - val_accuracy: 0.8953 - val_loss: 0.8124\n",
            "Epoch 39/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 63ms/step - accuracy: 0.9828 - loss: 0.0604 - val_accuracy: 0.8952 - val_loss: 0.8205\n",
            "Epoch 40/40\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - accuracy: 0.9834 - loss: 0.0577 - val_accuracy: 0.8954 - val_loss: 0.8304\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b6c9075f350>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGBKiUBXkV74"
      },
      "outputs": [],
      "source": [
        "# Simpan ke direktori\n",
        "transformer.save(\"/content/drive/MyDrive/Colab Notebooks/my_test_transformer_model.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6TzE2a7Urj8"
      },
      "source": [
        "\n",
        "## Decoding Test Sentences (Qualitative Analysis)\n",
        "\n",
        "Finally, let’s try translating new Indonesian sentences into English.\n",
        "\n",
        "To do this, we feed the model with a tokenized Indonesian input sentence and initialize the decoding process with the `\"[START]\"` token in the target sequence. The model will then predict the probability distribution of the next token.\n",
        "\n",
        "We continue generating tokens one at a time, using the tokens generated so far as context, until the model outputs the special `\"[END]\"` token, which signals the end of the translation.\n",
        "\n",
        "For the decoding strategy, we’ll use tools from the `keras_hub.samplers` module. In this example, we’ll apply **Greedy Decoding**, which selects the most likely next token (i.e., the one with the highest probability) at each step of the sequence generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVB2PCqNlR9D"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/my_test_transformer_model.keras\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TecgzFUpUrj9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9ed9614-41b1-4942-a4fa-04494fa4677f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Example 0 **\n",
            "i'll talk with you about this later, ok?\n",
            "aku akan membicarakan tentang hal ini , kamu tidak apa - ok ?\n",
            "\n",
            "** Example 1 **\n",
            "peel the apple.\n",
            "kupas apel itu !\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def decode_sequences(input_sentences):\n",
        "    batch_size = 1\n",
        "\n",
        "    # Tokenize the encoder input.\n",
        "    encoder_input_tokens = ops.convert_to_tensor(eng_tokenizer(input_sentences))\n",
        "\n",
        "    # Truncate or pad the sequence to MAX_SEQUENCE_LENGTH\n",
        "    encoder_input_tokens = encoder_input_tokens[:, :MAX_SEQUENCE_LENGTH]  # Truncate\n",
        "\n",
        "    # Define a function that outputs the next token's probability given the input sequence.\n",
        "    def next(prompt, cache, index):\n",
        "        logits = model([encoder_input_tokens, prompt])[:, index - 1, :]\n",
        "        # Ignore hidden states for now; only needed for contrastive search.\n",
        "        hidden_states = None\n",
        "        return logits, hidden_states, cache\n",
        "\n",
        "    # Build a prompt of length 40 with a start token and padding tokens.\n",
        "    length = 40\n",
        "    start = ops.full((batch_size, 1), ind_tokenizer.token_to_id(\"[START]\"))\n",
        "    pad = ops.full((batch_size, length - 1), ind_tokenizer.token_to_id(\"[PAD]\"))\n",
        "    prompt = ops.concatenate((start, pad), axis=-1)\n",
        "\n",
        "    generated_tokens = keras_hub.samplers.GreedySampler()(\n",
        "        next,\n",
        "        prompt,\n",
        "        stop_token_ids=[ind_tokenizer.token_to_id(\"[END]\")],\n",
        "        index=1,  # Start sampling after start token.\n",
        "    )\n",
        "    generated_sentences = ind_tokenizer.detokenize(generated_tokens)\n",
        "\n",
        "    # Return the first element of the generated sentences as a tensor\n",
        "    return generated_sentences[0]\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for i in range(2):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequences([input_sentence])\n",
        "    translated = (\n",
        "        translated.replace(\"[PAD]\", \"\")\n",
        "        .replace(\"[START]\", \"\")\n",
        "        .replace(\"[END]\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "    print(f\"** Example {i} **\")\n",
        "    print(input_sentence)\n",
        "    print(translated)\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}